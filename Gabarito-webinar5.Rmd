---
title: "Webinar 5 - Machine Learning"
subtitle: "Métodos Não Supervisionados: Clustering"
output: 
  html_document:
    code_download: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Sobre os dados

Para este webinar vamos usar 3 datasets: 

* O primeiro sobre a satisfação e lealdade de clientes;
* O segundo sobre compras de produtos (para realizar segmentação de mercado);
* O último dataset sobre salários de diferentes profissões (para agrupamento de profissões com crescimento de salários similares).

Você pode baixar os dados
```{r echo=FALSE}
xfun::pkg_load2(c('htmltools', 'mime'))
xfun::embed_dir("data5/", text = "aqui.")

```

## Carregando pacotes

```{r pacotes}

library(tidyverse)
library(dendextend) # para colorir os agrupamentos do cluster hirárquico
library(cluster) #para usar função pam
library(cowplot) #para plotar vários gráficos juntos

```


## Segmentação de clientes - Exemplo 1

Vamos começar com um exemplo simples de segmentação de clientes, usando apenas dois atributos: a satisfação do cliente e a sua lealdade.

```{r}

customers <- read.csv("data5/customers.csv")

customers %>% 
  ggplot(aes(Satisfaction, Loyalty))+
  geom_point(size=2)


customers <- scale(customers)

customers_df <- as.tibble(customers)

model <- kmeans(customers_df, centers = 2)

cluster_km <- model$cluster

customers_df %>% 
  mutate(cluster = cluster_km) %>% 
  ggplot(aes(Satisfaction, Loyalty, color = factor(cluster)))+
  geom_point(size=3)

```

Para escolher o número ideal de clusters, podemos usar o método do "Elbow". Para isto, vamos criar primeiramente uma função que facilite a criação de modelos k-means, logo iremos usar esta função `criar_k_means()` para gerar os valores das somas dos quadrados para diferentes valores de `k`.

```{r}

criar_k_means <- function(k){
  model <- kmeans(customers_df, centers = k)
  model$tot.withinss
}

k = 10

elbow <- data.frame(k=1:k)
elbow$tot.withinss <- 0


for(i in 1:k) {
  elbow$tot.withinss[i] <- criar_k_means(i)
}

elbow %>% 
  ggplot(aes(k, tot.withinss))+
  geom_line(color="red")+
  geom_point(size=4, color="red")+
  scale_x_continuous(breaks=1:10)

```

Similarmente, usaremos um segundo método para identificar o número ideal de cluster, o método da silueta.

```{r}
library(cluster)

criar_siluetas <- function(k){
  model <- pam(customers_df, k = k)
  model$silinfo$avg.width
}

k = 10

sil_df <- data.frame(k=2:k)
sil_df$sil_width <- 0


for(i in 2:k) {
  sil_df$sil_width[i-1] <- criar_siluetas(i)
}

sil_df %>% 
  ggplot(aes(k, sil_width))+
  geom_line(color="#e32d91")+
  geom_point(size=4, color="#e32d91")+
  scale_x_continuous(breaks=1:10)
```



## Segmentação de clientes - Exemplo 2

calcular a distância euclidiana entre clientes

```{r}
clientes <- readRDS("data5/ws_customers.rds")

dist_clientes <- dist(clientes)
```

criar uma análise usando o algoritmo da distância completa:

```{r}
ch_clientes <- hclust(dist_clientes, method = "complete")
```

graficar o dendograma:

```{r}
plot(ch_clientes)

```

Criar uma asignação de clusters no valor de altura h = 15000

```{r}
clust_clientes <- cutree(ch_clientes, h=15000)
```

Gerar um dataframe com os clientes segmentados

```{r}
clientes_segmentados <- mutate(clientes, cluster = clust_clientes)
```

Calcular o número de clientes que fazer parte de cada cluster

```{r}

count(clientes_segmentados, cluster)
```

colorir o dendograma com base no valor de cutoff

```{r}

dend_clientes <- as.dendrogram(ch_clientes)
dend_colorido <- color_branches(dend_clientes, h=15000)

plot(dend_colorido)
```


calcular a média para cada cluster

```{r}

clientes_segmentados %>% 
  group_by(cluster) %>% 
  summarise_all(mean)
```

Com base neste resultado, podemos inferir que:

* Os clientes do cluster 1 gastaram mais em Leite do que os outros grupos

* Os clientes do cluster 3 gastaram mais em mercado do que os outros grupos

* os clientes do cluster 4 gastaram mais em frozen iogurte do que os outros grupos

* Os clientes do cluster 2 gastaram consideravelmente menos do que os outros clusters



## Agrupamento de profissões a partir do crescimento dos salários

Vamos carregar os dados:

```{r oes}
oes <- readRDS("data5/oes.rds")
```

### Clustering hierárquico

passos necessários conforme o método

```{r}
# Calculate Euclidean distance between the occupations
dist_oes <- dist(oes, method = "euclidean")

# Generate an average linkage analysis 
hc_oes <- hclust(dist_oes, method = "average")

# Create a dendrogram object from the hclust variable
dend_oes <- as.dendrogram(hc_oes)

# Plot the dendrogram
plot(dend_oes)

# Color branches by cluster formed from the cut at a height of 100000
dend_colored <- color_branches(dend_oes, h = 100000)

# Plot the colored dendrogram
plot(dend_colored)
```


agora, vamos 'recortar' a árvore e vamos incluir o agrupamento num novo dataframe.

```{r}

# Use rownames_to_column to move the rownames into a column of the data frame
df_oes <- rownames_to_column(as.data.frame(oes), var = 'occupation')

# Create a cluster assignment vector at h = 100,000
cut_oes <- cutree(hc_oes, h = 100000)

# Generate the segmented the oes data frame
clust_oes <- mutate(df_oes, cluster = cut_oes)

# Create a tidy data frame by gathering the year and values into two columns
gathered_oes <- gather(data = clust_oes, 
                       key = year, 
                       value = mean_salary, 
                       -occupation, -cluster)

```

Agora vamos plotar os grupos de clusters para cada ano:

```{r}

# Plot the relationship between mean_salary and year and color the lines by the assigned cluster

clust3plot <- ggplot(gathered_oes, aes(x = year, y = mean_salary, color = factor(cluster))) + 
    geom_line(aes(group = occupation))+
  labs(title = "Clustering Hierárquico com k = 3")

clust3plot
```


### K-means

O algortimo do k-means segue a lógica descrita no último webinar. O primeiro passo é criar duas funções customizadas, que nos ajudarão a extrair as informações necessárias para a escolha do número de $k$ grupos.

```{r}

# Use map_dbl to run many models with varying value of k (centers)
tot_withinss <- map_dbl(1:10,  function(k){
  model <- kmeans(x = oes, centers = k)
  model$tot.withinss
})

# Generate a data frame containing both k and tot_withinss
elbow_df <- data.frame(
  k = 1:10,
  tot_withinss = tot_withinss
)

# Plot the elbow plot
ggplot(elbow_df, aes(x = k, y = tot_withinss)) +
  geom_line() +
  scale_x_continuous(breaks = 1:10)
```

O interessante é que o método do Elbow sugere 2 clusters, diferentemente do clustering hierárquico que sugeria 3.

```{r}
cluster_2 <- kmeans(oes, centers = 2)

clust2_oes <- mutate(df_oes, cluster = cluster_2$cluster)

clust2plot <- clust2_oes %>% 
  pivot_longer(2:16, names_to = "year", 
               values_to = "mean_salary") %>% 
  ggplot(aes(year, mean_salary, color = factor(cluster)))+
  geom_line(aes(group=occupation))+
  labs(title = "Kmeans com k = 2")

clust2plot

```


Vamos usar a largura média da silhueta (ASW) para tentar decidir qual número de clusters é o melhor.

```{r}


# Use map_dbl to run many models with varying value of k
sil_width <- map_dbl(2:10,  function(k){
  model <- pam(oes, k = k)
  model$silinfo$avg.width
})

# Generate a data frame containing both k and sil_width
sil_df <- data.frame(
  k = 2:10,
  sil_width = sil_width
)

# Plot the relationship between k and sil_width
ggplot(sil_df, aes(x = k, y = sil_width)) +
  geom_line(color = "#e32d91", size =1) +
  scale_x_continuous(breaks = 2:10)

```

Neste método, quanto mais alto for o valor de um referido cluster, melhor, ou seja, este método sugere que o melhor número de clusters seria 7, pois obteve o valor mais alto de largura, seguido pelo cluster k = 2.

```{r}

cluster_7 <- kmeans(oes, centers = 7)

clust7_oes <- mutate(df_oes, cluster = cluster_7$cluster)

clust7plot <- clust7_oes %>% 
  pivot_longer(2:16, names_to = "year", 
               values_to = "mean_salary") %>% 
  ggplot(aes(year, mean_salary, color = factor(cluster)))+
  geom_line(aes(group=occupation))+
  labs(title = "Kmeans com k = 7")

clust7plot
```


Vamos comparar os três resultados:

```{r fig.width=14, fig.height=4}


plot_grid(clust2plot, clust3plot, clust7plot, ncol = 3, label_size = 10)
```

